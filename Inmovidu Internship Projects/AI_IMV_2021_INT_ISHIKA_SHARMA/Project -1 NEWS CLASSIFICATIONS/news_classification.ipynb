{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdAFCEHLLnfL"
   },
   "source": [
    "# **ARTIFICIAL INTELLIGENCE**\n",
    "## **_PROJECT-1_**\n",
    "### **_NEWS CLASSIFICATION_**\n",
    "\n",
    "`NAME:` **_ISHIKA SHARMA_**\n",
    "\n",
    "`EMAIL ID:` ishikasharma.aug2001@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GyxcKBWfwg28",
    "outputId": "6646e49a-3d3f-4cad-9d6d-29ed314d9b88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oy4y4ospww27",
    "outputId": "1671a0de-f091-4bab-c8bc-eac210cc149a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'target'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#reading the files into dataframes\n",
    "fake = pd.read_csv(\"Fake.csv\")\n",
    "genuine = pd.read_csv(\"True.csv\")\n",
    "\n",
    "#adding a column 'target' to tell the result to aour model\n",
    "fake['target'] = 0\n",
    "genuine['target'] = 1\n",
    "\n",
    "#concatenating data to make a single dataFrame\n",
    "data = pd.concat([fake, genuine], axis=0)\n",
    "\n",
    "#resetting the index of entire dataset\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "#dropping extra columns\n",
    "data = data.drop(['subject','date','title'], axis=1)\n",
    "\n",
    "#check data\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ge-cMQPLh9s"
   },
   "source": [
    "### TOKENIZATION\n",
    "Divides a large piece of continuous text into distinct units or tokens basically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jrul8F_7LhLY",
    "outputId": "ffcec46b-dfe9-4c10-dc15-97702ae5440a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target\n",
      "0  [Donald, Trump, just, couldn, t, wish, all, Am...       0\n",
      "1  [House, Intelligence, Committee, Chairman, Dev...       0\n",
      "2  [On, Friday, ,, it, was, revealed, that, forme...       0\n",
      "3  [On, Christmas, day, ,, Donald, Trump, announc...       0\n",
      "4  [Pope, Francis, used, his, annual, Christmas, ...       0\n",
      "5  [The, number, of, cases, of, cops, brutalizing...       0\n",
      "6  [Donald, Trump, spent, a, good, portion, of, h...       0\n",
      "7  [In, the, wake, of, yet, another, court, decis...       0\n",
      "8  [Many, people, have, raised, the, alarm, regar...       0\n",
      "9  [Just, when, you, might, have, thought, we, d,...       0\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize #, sent_tokenize\n",
    "\n",
    "data['text'] = data['text'].apply(word_tokenize)\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gu28S7RiO8I1"
   },
   "source": [
    "### STEMMING\n",
    "The idea of removing the suffix of a word and reducing different forms of a word to a core root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7xLap_4-O_Fj",
    "outputId": "2c9ff6c1-c998-44a7-eb91-60dae47ec8ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target\n",
      "0  [donald, trump, just, couldn, t, wish, all, am...       0\n",
      "1  [hous, intellig, committe, chairman, devin, nu...       0\n",
      "2  [on, friday, ,, it, was, reveal, that, former,...       0\n",
      "3  [on, christma, day, ,, donald, trump, announc,...       0\n",
      "4  [pope, franci, use, his, annual, christma, day...       0\n",
      "5  [the, number, of, case, of, cop, brutal, and, ...       0\n",
      "6  [donald, trump, spent, a, good, portion, of, h...       0\n",
      "7  [in, the, wake, of, yet, anoth, court, decis, ...       0\n",
      "8  [mani, peopl, have, rais, the, alarm, regard, ...       0\n",
      "9  [just, when, you, might, have, thought, we, d,...       0\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "porter = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_it(text):\n",
    "  return [porter.stem(word) for word in text]\n",
    "\n",
    "data['text'] = data['text'].apply(stem_it)\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdXaHvCbQhjd"
   },
   "source": [
    "### STOPWORD REMOVAL\n",
    "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NyJOdaATFPB"
   },
   "outputs": [],
   "source": [
    "#first way\n",
    "#from nltk.corpus import stopwords\n",
    "#print(stopwords.words('eglish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRbIdj0BQkmD",
    "outputId": "08ca2eb5-8d6a-4bf2-f355-2833d99cc740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target\n",
      "0  [donald, trump, just, couldn, wish, all, ameri...       0\n",
      "1  [hous, intellig, committe, chairman, devin, nu...       0\n",
      "2  [friday, was, reveal, that, former, milwauke, ...       0\n",
      "3  [christma, day, donald, trump, announc, that, ...       0\n",
      "4  [pope, franci, use, his, annual, christma, day...       0\n",
      "5  [the, number, case, cop, brutal, and, kill, pe...       0\n",
      "6  [donald, trump, spent, good, portion, his, day...       0\n",
      "7  [the, wake, yet, anoth, court, decis, that, de...       0\n",
      "8  [mani, peopl, have, rais, the, alarm, regard, ...       0\n",
      "9  [just, when, you, might, have, thought, get, b...       0\n"
     ]
    }
   ],
   "source": [
    "#Another way of stemming\n",
    "def stop_it(t):\n",
    "  dt = [word for word in t if len(word) > 2]\n",
    "  return dt\n",
    "\n",
    "data['text'] = data['text'].apply(stop_it)\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTBxXqPHRj8c",
    "outputId": "9a17ae1c-a792-4819-d13c-7e5f50af8e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target\n",
      "0  donald trump just couldn wish all american hap...       0\n",
      "1  hous intellig committe chairman devin nune hav...       0\n",
      "2  friday was reveal that former milwauke sheriff...       0\n",
      "3  christma day donald trump announc that would b...       0\n",
      "4  pope franci use his annual christma day messag...       0\n",
      "5  the number case cop brutal and kill peopl colo...       0\n",
      "6  donald trump spent good portion his day his go...       0\n",
      "7  the wake yet anoth court decis that derail don...       0\n",
      "8  mani peopl have rais the alarm regard the fact...       0\n",
      "9  just when you might have thought get break fro...       0\n"
     ]
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(' '.join)\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8vAMf73R6jn"
   },
   "source": [
    "### SPLITTING dataset for training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8fDXhDs0R8ke"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['target'], test_size=0.25)\n",
    "\n",
    "# display(X_train.head(10))\n",
    "# print('\\n')\n",
    "# display(y_train.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mr6ab744Uc49"
   },
   "source": [
    "### VECTORIZATION\n",
    "\n",
    "The vectorization is a technique used to convert textual data to numerical format. Using vectorization, a matrix is created where each column represents a feature and each row represents an individual review.\n",
    "\n",
    "#### _TF (Term Frequency)_\n",
    "\n",
    "Term Frequency is defined as how frequently the word appear  in the document.\n",
    "\n",
    "#### _Term Frequency-Inverse Document Frequency(TF-IDF)_\n",
    "\n",
    "TD-IDF  basically tells importance of the word in the corpus or dataset\n",
    "- It is the combination of Term frequency and Inverse Document Frequency. \n",
    "- Inverse Document frequency is another concept which is used for finding out importance of the word. It is based on the fact that less frequent words are more informative and important.\n",
    "\n",
    "__IDF(t) = log_e(Total number of documents / Number of documents with term t in it)__\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DxnjdXH4Uew9",
    "outputId": "65c6b2c6-2f6f-47e1-e2aa-d2cabd03c43c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 87044)\t0.022145025191346538\n",
      "  (0, 17945)\t0.0369924568867777\n",
      "  (0, 73398)\t0.0384114481757907\n",
      "  (0, 41051)\t0.023598486798303443\n",
      "  (0, 31388)\t0.024787670870778336\n",
      "  (0, 81455)\t0.020931363444334314\n",
      "  (0, 12263)\t0.06766956891071564\n",
      "  (0, 36790)\t0.037904780260715885\n",
      "  (0, 11295)\t0.045599295727874536\n",
      "  (0, 11732)\t0.054265897911665834\n",
      "  (0, 63171)\t0.08354541137314743\n",
      "  (0, 22108)\t0.04739412271770294\n",
      "  (0, 66856)\t0.025985293181999148\n",
      "  (0, 31242)\t0.03579136278425623\n",
      "  (0, 9886)\t0.020992245344637648\n",
      "  (0, 76586)\t0.0411444879923586\n",
      "  (0, 61503)\t0.02063179534053521\n",
      "  (0, 26780)\t0.024975066271438487\n",
      "  (0, 60083)\t0.054463048008756544\n",
      "  (0, 57498)\t0.038972552367131534\n",
      "  (0, 87286)\t0.03615113404541332\n",
      "  (0, 65357)\t0.02679535657201108\n",
      "  (0, 81430)\t0.04971184069552565\n",
      "  (0, 55143)\t0.049854107569574285\n",
      "  (0, 48989)\t0.052962090943056164\n",
      "  :\t:\n",
      "  (33672, 85467)\t0.06411896595956618\n",
      "  (33672, 30037)\t0.0687000305930678\n",
      "  (33672, 51264)\t0.04362551842361566\n",
      "  (33672, 17565)\t0.038062811400065164\n",
      "  (33672, 54779)\t0.054929060840897555\n",
      "  (33672, 80179)\t0.19825822084009548\n",
      "  (33672, 48811)\t0.04655080960243465\n",
      "  (33672, 42886)\t0.04673840189417719\n",
      "  (33672, 17092)\t0.05501163749511788\n",
      "  (33672, 24399)\t0.05402516197526952\n",
      "  (33672, 30376)\t0.09289199993856978\n",
      "  (33672, 57822)\t0.06911947205750232\n",
      "  (33672, 69903)\t0.03130672919803619\n",
      "  (33672, 22401)\t0.03879388864318645\n",
      "  (33672, 26788)\t0.0343005148200037\n",
      "  (33672, 88114)\t0.04160213687098757\n",
      "  (33672, 70299)\t0.04051564571562802\n",
      "  (33672, 20192)\t0.06452054710069001\n",
      "  (33672, 55556)\t0.1044573344708973\n",
      "  (33672, 66140)\t0.28305163728478466\n",
      "  (33672, 34780)\t0.04064981082572599\n",
      "  (33672, 49664)\t0.036086905545312714\n",
      "  (33672, 60894)\t0.040887021830097514\n",
      "  (33672, 17138)\t0.02740415169242477\n",
      "  (33672, 8087)\t0.09585890527390419\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "my_tfidf = TfidfVectorizer(max_df=0.7)\n",
    "\n",
    "tfidf_train = my_tfidf.fit_transform(X_train)\n",
    "tfidf_test = my_tfidf.transform(X_test)\n",
    "\n",
    "print(tfidf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSDhHW0fZGhn"
   },
   "source": [
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vy6o6_KiZFt2",
    "outputId": "0d34308d-b59e-44c7-eece-ff9ca454fff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.80623608017818\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_1 = LogisticRegression(max_iter=900)\n",
    "model_1.fit(tfidf_train, y_train)\n",
    "\n",
    "pred_1 = model_1.predict(tfidf_test)\n",
    "cr1 = accuracy_score(y_test, pred_1)\n",
    "print(cr1*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rXzBQUAaj-H"
   },
   "source": [
    "### PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OjpqJnfjaoR_",
    "outputId": "6953f21c-2b8c-4443-fa2e-99611d726f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of prediction is:  99.60801781737194\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "model = PassiveAggressiveClassifier(max_iter=50)\n",
    "model.fit(tfidf_train, y_train)\n",
    "\n",
    "y_pred = model.predict(tfidf_test)\n",
    "accscore = accuracy_score(y_test, y_pred)\n",
    "print('The accuracy of prediction is: ', accscore*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_WHiyZuim3A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "news_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
